{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e403711-2ae0-4b50-ab15-78b08dced4f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "########I have around 9.5 years of IT experience, with 5.5+ years in Big Data technologies and Big Data Testing. I have worked extensively with Hadoop, Hive, Sqoop, Spark, and PySpark, and I specialize in ETL/DWH and Big Data testing.\n",
    "########In my last project, I worked on end-to-end data pipeline testing, which was built on the Hadoop Medallion Architecture. The architecture was divided into Raw (Bronze), Silver, and Gold layers, and I was responsible for end-to-end data validation, including data quality, transformation accuracy, and reconciliation across all layers.\n",
    "########We received data from multiple source systems such as SAP, CCS, HLS, and CSV files. Initially, the incoming files were landed in a landing path on the Edge Node. From there, the files were moved to HDFS using a file transfer job, also known as a data watcher job.\n",
    "########Once the data reached the Raw (Bronze) layer in Hadoop, I performed sanity and delta testing, which included:\n",
    "‚Ä¢\tFile format validation\n",
    "‚Ä¢\tFile naming convention checks\n",
    "‚Ä¢\tZero-byte file validation\n",
    "‚Ä¢\tDuplicate file name checks\n",
    "‚Ä¢\tDifferent file names with duplicate records\n",
    "After successful raw layer validation, a master scheduler job triggered downstream processing. Based on the upstream files, staging tables were created.\n",
    "From staging, the data was transformed and loaded into the Silver layer, where fact and dimension tables were populated. I validated:\n",
    "‚Ä¢\tSource-to-target record counts\n",
    "‚Ä¢\tBusiness transformation logic\n",
    "‚Ä¢\tData type and NULL handling\n",
    "‚Ä¢\tDuplicate records\n",
    "The Gold layer acted as the consumption layer, where business views were created. These views were consumed by BI tools, visualization teams, and data science teams. I ensured the data was accurate, aggregated correctly, and aligned with business requirements.\n",
    "For data governance and compliance, data in the Gold layer was archived and secured using service accounts, and access was controlled as per governance standards.\n",
    "Overall, my role involved validating the complete data flow from source systems to consumption, ensuring high data quality, accuracy, and reliability across the Medallion Architecture.#####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bd6b288-6a75-41ef-aee7-8ec4b06ee7b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## üë®‚Äçüíª Professional Summary\n",
    "\n",
    "I have around **9.5 years of IT experience**, with **5.5+ years in Big Data technologies and Big Data Testing**. I have worked extensively with **Hadoop, Hive, Sqoop, Spark, and PySpark**, and I specialize in **ETL/DWH and Big Data testing**.\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Project Overview ‚Äì Medallion Architecture\n",
    "\n",
    "In my **last project**, I worked on **end-to-end data pipeline testing**, which was built on the **Hadoop Medallion Architecture**.  \n",
    "The architecture consisted of **Raw (Bronze), Silver, and Gold layers**, and I was responsible for **end-to-end data validation**, including **data quality, transformation accuracy, and reconciliation across all layers**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Data Ingestion Flow\n",
    "\n",
    "We received data from multiple source systems such as:\n",
    "- **SAP**\n",
    "- **CCS**\n",
    "- **HLS**\n",
    "- **CSV files**\n",
    "\n",
    "Initially, the incoming files were landed in a **landing path on the Edge Node**.  \n",
    "From there, the files were moved to **HDFS** using a **file transfer job**, also known as a **data watcher job**.\n",
    "\n",
    "---\n",
    "\n",
    "## ü•â Raw (Bronze) Layer Validation\n",
    "\n",
    "Once the data reached the **Raw (Bronze) layer** in Hadoop, I performed **sanity and delta testing**, which included:\n",
    "\n",
    "- File format validation  \n",
    "- File naming convention checks  \n",
    "- Zero-byte file validation  \n",
    "- Duplicate file name checks  \n",
    "- Different file names with duplicate records  \n",
    "\n",
    "After successful raw layer validation, a **master scheduler job** triggered downstream processing.  \n",
    "Based on the upstream files, **staging tables** were created.\n",
    "\n",
    "---\n",
    "\n",
    "## ü•à Silver Layer Validation\n",
    "\n",
    "From the staging layer, the data was transformed and loaded into the **Silver layer**, where **fact and dimension tables** were populated.\n",
    "\n",
    "I validated:\n",
    "- Source-to-target record counts  \n",
    "- Business transformation logic  \n",
    "- Data type and NULL handling  \n",
    "- Duplicate records  \n",
    "\n",
    "---\n",
    "\n",
    "## ü•á Gold Layer (Consumption Layer)\n",
    "\n",
    "The **Gold layer** acted as the **consumption layer**, where **business views** were created.  \n",
    "These views were consumed by:\n",
    "- BI tools  \n",
    "- Visualization teams  \n",
    "- Data science teams  \n",
    "\n",
    "I ensured that the data was **accurate, properly aggregated, and aligned with business requirements**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîê Data Governance & Compliance\n",
    "\n",
    "For **data governance and compliance**, data in the Gold layer was:\n",
    "- Archived using **service accounts**\n",
    "- Secured with **controlled access**\n",
    "- Managed as per governance standards\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Overall Responsibility\n",
    "\n",
    "Overall, my role involved **validating the complete data flow from source systems to consumption**, ensuring **high data quality, accuracy, and reliability** across the **Medallion Architecture**.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Testing_self_intro",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
