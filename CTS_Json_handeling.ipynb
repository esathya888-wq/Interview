{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8459d09e-0805-4853-9487-11cb1afae36e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ”¥ Data Handling â€“ JSON | CSV | Parquet (Hive & PySpark)\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§± File Types Used in Big Data Pipelines\n",
    "- JSON (simple, nested)\n",
    "- CSV (delimited)\n",
    "- Parquet (columnar)\n",
    "\n",
    "---\n",
    "\n",
    "### =======================\n",
    "###ðŸŸ¢ JSON HANDLING\n",
    "###=======================\n",
    "\n",
    "### JSON Examples\n",
    "\n",
    "### Simple JSON\n",
    "```json\n",
    "{\"id\":1,\"name\":\"Sathya\",\"age\":30}\n",
    "```\n",
    "### Nested JSON\n",
    "```\n",
    "{\n",
    "  \"emp\": {\n",
    "    \"id\": 101,\n",
    "    \"name\": \"Arun\",\n",
    "    \"skills\": [\"Hive\",\"Spark\"],\n",
    "    \"address\": {\n",
    "      \"city\": \"Chennai\",\n",
    "      \"state\": \"TN\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "### JSON in Hive\n",
    "####Create Table (Schema-on-Read)\n",
    "```\n",
    "CREATE TABLE raw_json (\n",
    "  emp STRUCT<\n",
    "    id:INT,\n",
    "    name:STRING,\n",
    "    skills:ARRAY<STRING>,\n",
    "    address:STRUCT<city:STRING,state:STRING>\n",
    "  >\n",
    ")\n",
    "ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe';\n",
    "```\n",
    "\n",
    "####Extract Nested Fields\n",
    "```\n",
    "SELECT\n",
    "  emp.id,\n",
    "  emp.name,\n",
    "  emp.address.city,\n",
    "  emp.address.state\n",
    "FROM raw_json;\n",
    "```\n",
    "#### get_json_object (String JSON)\n",
    "```\n",
    "SELECT\n",
    "  get_json_object(json_col,'$.emp.id')   AS emp_id,\n",
    "  get_json_object(json_col,'$.emp.name') AS emp_name\n",
    "FROM raw_json;\n",
    "```\n",
    "\n",
    "####Flatten Array â€“ EXPLODE\n",
    "```\n",
    "SELECT\n",
    "  emp.id,\n",
    "  skill\n",
    "FROM raw_json\n",
    "LATERAL VIEW EXPLODE(emp.skills) t AS skill;\n",
    "```\n",
    "####Flatten Array â€“ POSEXPLODE\n",
    "```\n",
    "SELECT\n",
    "  emp.id,\n",
    "  pos,\n",
    "  skill\n",
    "FROM raw_json\n",
    "LATERAL VIEW POSEXPLODE(emp.skills) t AS pos, skill;\n",
    "```\n",
    "###JSON in PySpark\n",
    "```\n",
    "df = spark.read.option(\"multiLine\",\"true\").json(\"/path/json\")\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "```\n",
    "####Select Nested Fields\n",
    "```\n",
    "df.select(\n",
    "  df.emp.id.alias(\"emp_id\"),\n",
    "  df.emp.name.alias(\"emp_name\"),\n",
    "  df.emp.address.city.alias(\"city\")\n",
    ").show()\n",
    "```\n",
    "####Flatten Array\n",
    "```\n",
    "from pyspark.sql.functions import explode\n",
    "df.select(df.emp.id, explode(df.emp.skills)).show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e72e2387-b7e3-44f6-8267-1d84ef350730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.head(\"dbfs:/Volumes/dev/club_db/data/json/nested_json.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22d519f2-d109-4c59-bbe0-c3b0c8ab885e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read JSON and save as table"
    }
   },
   "outputs": [],
   "source": [
    "# Minimal fix: Use Python to read JSON from Unity Catalog volume and save as table\n",
    "json_path = \"/Volumes/dev/club_db/data/json/simple_json.json\"\n",
    "df = spark.read.json(json_path)\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"dev.club_db.bronze_simple_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ee2957d-8434-49bf-98a7-6445410332a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from dev.club_db.bronze_simple_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af646d9b-716e-4056-a474-060711251fa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW raw_employees AS\n",
    "SELECT *\n",
    "FROM read_files(\n",
    "  \"/Volumes/dev/club_db/data/json/nested_json.json\",\n",
    "  format => \"json\",\n",
    "  multiline => true\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e21b512-d18b-4c37-9783-6ec7337b55e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "260c93ab-4857-4e6f-9c32-43d1c6028c14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from raw_employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "213f34b0-e3d0-4503-925d-ba173c36f74f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 8"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  emp.address.city  AS city,\n",
    "  emp.address.state AS state,\n",
    "  skill\n",
    "FROM raw_employees\n",
    "LATERAL VIEW EXPLODE(employees) e AS emp\n",
    "LATERAL VIEW EXPLODE(emp.skills) s AS skill;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfe50e0c-aeff-4d3c-9e08-a6cfbadacf33",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  emp.address.city   AS emp_city,\n",
    "  emp.address.state AS emp_state\n",
    "FROM raw_employees;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7296d8d-42c4-4503-8071-912e202ab748",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 4"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, LongType, StringType\n",
    "\n",
    "json_path1 = \"/Volumes/dev/club_db/data/json/nested_json.json\"\n",
    "schema = StructType([\n",
    "    StructField('age', LongType(), True),\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField('name', StringType(), True)\n",
    "])\n",
    "df1 = spark.read.schema(schema).option('mode', 'PERMISSIVE').json(json_path1)\n",
    "df1.write.saveAsTable(\"dev.club_db.bronze_nested_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dff32829-1f01-46fb-969f-6a74717d9292",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select *  from dev.club_db.bronze_nested_json"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4882955036442135,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "CTS_Json_handeling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
